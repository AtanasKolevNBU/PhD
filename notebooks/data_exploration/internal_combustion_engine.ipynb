{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\Repositories\\PhD\\parsed_data\\ICE_data\\internal_combustion_engine_bearings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Demand 1</th>\n",
       "      <th>Control 1</th>\n",
       "      <th>Output Drive 1</th>\n",
       "      <th>Channel 1</th>\n",
       "      <th>Channel 2</th>\n",
       "      <th>Channel 3</th>\n",
       "      <th>Channel 4</th>\n",
       "      <th>Channel 1 Kurtosis</th>\n",
       "      <th>Channel 2 Kurtosis</th>\n",
       "      <th>Channel 3 Kurtosis</th>\n",
       "      <th>Channel 4 Kurtosis</th>\n",
       "      <th>Rear Input 1</th>\n",
       "      <th>Rear Input 2</th>\n",
       "      <th>Rear Input 3</th>\n",
       "      <th>Rear Input 4</th>\n",
       "      <th>Rear Input 5</th>\n",
       "      <th>Rear Input 6</th>\n",
       "      <th>Rear Input 7</th>\n",
       "      <th>Rear Input 8</th>\n",
       "      <th>FAULT</th>\n",
       "      <th>RPM</th>\n",
       "      <th>HUMIDITY%</th>\n",
       "      <th>TEMPERATURE_Celsius</th>\n",
       "      <th>resultant_vibration_magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.125011</td>\n",
       "      <td>0.176033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211458</td>\n",
       "      <td>0.209182</td>\n",
       "      <td>0.145823</td>\n",
       "      <td>1.624200e-15</td>\n",
       "      <td>2.52457</td>\n",
       "      <td>2.94874</td>\n",
       "      <td>1.83760</td>\n",
       "      <td>7.52253</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.331264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.125011</td>\n",
       "      <td>0.176033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211458</td>\n",
       "      <td>0.209182</td>\n",
       "      <td>0.145823</td>\n",
       "      <td>1.624200e-15</td>\n",
       "      <td>2.52457</td>\n",
       "      <td>2.94874</td>\n",
       "      <td>1.83760</td>\n",
       "      <td>7.52253</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.331264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.125011</td>\n",
       "      <td>0.176033</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.206329</td>\n",
       "      <td>0.206513</td>\n",
       "      <td>0.150478</td>\n",
       "      <td>1.643320e-15</td>\n",
       "      <td>2.32290</td>\n",
       "      <td>2.46553</td>\n",
       "      <td>1.76090</td>\n",
       "      <td>6.51397</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.328425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009633</td>\n",
       "      <td>0.125011</td>\n",
       "      <td>0.172626</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.206351</td>\n",
       "      <td>0.194663</td>\n",
       "      <td>0.148313</td>\n",
       "      <td>1.528270e-15</td>\n",
       "      <td>2.26458</td>\n",
       "      <td>2.55488</td>\n",
       "      <td>1.79255</td>\n",
       "      <td>4.13510</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.320111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.125011</td>\n",
       "      <td>0.172626</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.214463</td>\n",
       "      <td>0.214489</td>\n",
       "      <td>0.155652</td>\n",
       "      <td>1.791370e-15</td>\n",
       "      <td>2.53380</td>\n",
       "      <td>2.66379</td>\n",
       "      <td>1.78517</td>\n",
       "      <td>8.11885</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.340921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390258</th>\n",
       "      <td>27.302300</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.515140</td>\n",
       "      <td>0.043543</td>\n",
       "      <td>0.554265</td>\n",
       "      <td>0.560519</td>\n",
       "      <td>1.633510</td>\n",
       "      <td>1.712510e-15</td>\n",
       "      <td>3.10602</td>\n",
       "      <td>3.09644</td>\n",
       "      <td>2.91947</td>\n",
       "      <td>6.77653</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>1.813766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390259</th>\n",
       "      <td>27.305700</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.515140</td>\n",
       "      <td>0.043541</td>\n",
       "      <td>0.554672</td>\n",
       "      <td>0.560996</td>\n",
       "      <td>1.646360</td>\n",
       "      <td>1.711110e-15</td>\n",
       "      <td>3.10423</td>\n",
       "      <td>3.09293</td>\n",
       "      <td>2.90529</td>\n",
       "      <td>6.77717</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>1.825617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390260</th>\n",
       "      <td>27.309200</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.515140</td>\n",
       "      <td>0.043536</td>\n",
       "      <td>0.555249</td>\n",
       "      <td>0.561109</td>\n",
       "      <td>1.645410</td>\n",
       "      <td>1.710140e-15</td>\n",
       "      <td>3.11372</td>\n",
       "      <td>3.09118</td>\n",
       "      <td>2.89499</td>\n",
       "      <td>6.75728</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>1.824971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390261</th>\n",
       "      <td>27.312500</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.515305</td>\n",
       "      <td>0.043532</td>\n",
       "      <td>0.555029</td>\n",
       "      <td>0.560672</td>\n",
       "      <td>1.640060</td>\n",
       "      <td>1.711640e-15</td>\n",
       "      <td>3.11269</td>\n",
       "      <td>3.09097</td>\n",
       "      <td>2.90542</td>\n",
       "      <td>6.77779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>1.819947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390262</th>\n",
       "      <td>27.315800</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.515305</td>\n",
       "      <td>0.043526</td>\n",
       "      <td>0.554786</td>\n",
       "      <td>0.560474</td>\n",
       "      <td>1.632540</td>\n",
       "      <td>1.710060e-15</td>\n",
       "      <td>3.11065</td>\n",
       "      <td>3.09005</td>\n",
       "      <td>2.92384</td>\n",
       "      <td>6.77203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>1.813038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>390263 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time  Demand 1  Control 1  Output Drive 1  Channel 1  Channel 2  \\\n",
       "0        0.001450  0.125011   0.176033        0.000000   0.211458   0.209182   \n",
       "1        0.001450  0.125011   0.176033        0.000000   0.211458   0.209182   \n",
       "2        0.006283  0.125011   0.176033        0.000012   0.206329   0.206513   \n",
       "3        0.009633  0.125011   0.172626        0.000013   0.206351   0.194663   \n",
       "4        0.013200  0.125011   0.172626        0.000014   0.214463   0.214489   \n",
       "...           ...       ...        ...             ...        ...        ...   \n",
       "390258  27.302300  0.500044   0.515140        0.043543   0.554265   0.560519   \n",
       "390259  27.305700  0.500044   0.515140        0.043541   0.554672   0.560996   \n",
       "390260  27.309200  0.500044   0.515140        0.043536   0.555249   0.561109   \n",
       "390261  27.312500  0.500044   0.515305        0.043532   0.555029   0.560672   \n",
       "390262  27.315800  0.500044   0.515305        0.043526   0.554786   0.560474   \n",
       "\n",
       "        Channel 3     Channel 4  Channel 1 Kurtosis  Channel 2 Kurtosis  \\\n",
       "0        0.145823  1.624200e-15             2.52457             2.94874   \n",
       "1        0.145823  1.624200e-15             2.52457             2.94874   \n",
       "2        0.150478  1.643320e-15             2.32290             2.46553   \n",
       "3        0.148313  1.528270e-15             2.26458             2.55488   \n",
       "4        0.155652  1.791370e-15             2.53380             2.66379   \n",
       "...           ...           ...                 ...                 ...   \n",
       "390258   1.633510  1.712510e-15             3.10602             3.09644   \n",
       "390259   1.646360  1.711110e-15             3.10423             3.09293   \n",
       "390260   1.645410  1.710140e-15             3.11372             3.09118   \n",
       "390261   1.640060  1.711640e-15             3.11269             3.09097   \n",
       "390262   1.632540  1.710060e-15             3.11065             3.09005   \n",
       "\n",
       "        Channel 3 Kurtosis  Channel 4 Kurtosis  Rear Input 1  Rear Input 2  \\\n",
       "0                  1.83760             7.52253             0             0   \n",
       "1                  1.83760             7.52253             0             0   \n",
       "2                  1.76090             6.51397             0             0   \n",
       "3                  1.79255             4.13510             0             0   \n",
       "4                  1.78517             8.11885             0             0   \n",
       "...                    ...                 ...           ...           ...   \n",
       "390258             2.91947             6.77653             0             0   \n",
       "390259             2.90529             6.77717             0             0   \n",
       "390260             2.89499             6.75728             0             0   \n",
       "390261             2.90542             6.77779             0             0   \n",
       "390262             2.92384             6.77203             0             0   \n",
       "\n",
       "        Rear Input 3  Rear Input 4  Rear Input 5  Rear Input 6  Rear Input 7  \\\n",
       "0                  0             0             0             0             0   \n",
       "1                  0             0             0             0             0   \n",
       "2                  0             0             0             0             0   \n",
       "3                  0             0             0             0             0   \n",
       "4                  0             0             0             0             0   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "390258             0             0             0             0             0   \n",
       "390259             0             0             0             0             0   \n",
       "390260             0             0             0             0             0   \n",
       "390261             0             0             0             0             0   \n",
       "390262             0             0             0             0             0   \n",
       "\n",
       "        Rear Input 8  FAULT   RPM  HUMIDITY%  TEMPERATURE_Celsius  \\\n",
       "0                  0      1  1000          0                  -10   \n",
       "1                  0      1  1000          0                  -10   \n",
       "2                  0      1  1000          0                  -10   \n",
       "3                  0      1  1000          0                  -10   \n",
       "4                  0      1  1000          0                  -10   \n",
       "...              ...    ...   ...        ...                  ...   \n",
       "390258             0      0  2000         50                   45   \n",
       "390259             0      0  2000         50                   45   \n",
       "390260             0      0  2000         50                   45   \n",
       "390261             0      0  2000         50                   45   \n",
       "390262             0      0  2000         50                   45   \n",
       "\n",
       "        resultant_vibration_magnitude  \n",
       "0                            0.331264  \n",
       "1                            0.331264  \n",
       "2                            0.328425  \n",
       "3                            0.320111  \n",
       "4                            0.340921  \n",
       "...                               ...  \n",
       "390258                       1.813766  \n",
       "390259                       1.825617  \n",
       "390260                       1.824971  \n",
       "390261                       1.819947  \n",
       "390262                       1.813038  \n",
       "\n",
       "[390263 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resultant vibration magnitude is given by $ \\sqrt{x^2 + y^2 + z^2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['resultant_vibration_magnitude'] = np.sqrt(df['Channel 1']**2 + df['Channel 2']**2 + df['Channel 3']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Demand 1</th>\n",
       "      <th>Control 1</th>\n",
       "      <th>Output Drive 1</th>\n",
       "      <th>Channel 1</th>\n",
       "      <th>Channel 2</th>\n",
       "      <th>Channel 3</th>\n",
       "      <th>Channel 4</th>\n",
       "      <th>Channel 1 Kurtosis</th>\n",
       "      <th>Channel 2 Kurtosis</th>\n",
       "      <th>Channel 3 Kurtosis</th>\n",
       "      <th>Channel 4 Kurtosis</th>\n",
       "      <th>Rear Input 1</th>\n",
       "      <th>Rear Input 2</th>\n",
       "      <th>Rear Input 3</th>\n",
       "      <th>Rear Input 4</th>\n",
       "      <th>Rear Input 5</th>\n",
       "      <th>Rear Input 6</th>\n",
       "      <th>Rear Input 7</th>\n",
       "      <th>Rear Input 8</th>\n",
       "      <th>FAULT</th>\n",
       "      <th>RPM</th>\n",
       "      <th>HUMIDITY%</th>\n",
       "      <th>TEMPERATURE_Celsius</th>\n",
       "      <th>resultant_vibration_magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.125011</td>\n",
       "      <td>0.176033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211458</td>\n",
       "      <td>0.209182</td>\n",
       "      <td>0.145823</td>\n",
       "      <td>1.624200e-15</td>\n",
       "      <td>2.52457</td>\n",
       "      <td>2.94874</td>\n",
       "      <td>1.83760</td>\n",
       "      <td>7.52253</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.331264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.125011</td>\n",
       "      <td>0.176033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211458</td>\n",
       "      <td>0.209182</td>\n",
       "      <td>0.145823</td>\n",
       "      <td>1.624200e-15</td>\n",
       "      <td>2.52457</td>\n",
       "      <td>2.94874</td>\n",
       "      <td>1.83760</td>\n",
       "      <td>7.52253</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.331264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.125011</td>\n",
       "      <td>0.176033</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.206329</td>\n",
       "      <td>0.206513</td>\n",
       "      <td>0.150478</td>\n",
       "      <td>1.643320e-15</td>\n",
       "      <td>2.32290</td>\n",
       "      <td>2.46553</td>\n",
       "      <td>1.76090</td>\n",
       "      <td>6.51397</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.328425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009633</td>\n",
       "      <td>0.125011</td>\n",
       "      <td>0.172626</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.206351</td>\n",
       "      <td>0.194663</td>\n",
       "      <td>0.148313</td>\n",
       "      <td>1.528270e-15</td>\n",
       "      <td>2.26458</td>\n",
       "      <td>2.55488</td>\n",
       "      <td>1.79255</td>\n",
       "      <td>4.13510</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.320111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.125011</td>\n",
       "      <td>0.172626</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.214463</td>\n",
       "      <td>0.214489</td>\n",
       "      <td>0.155652</td>\n",
       "      <td>1.791370e-15</td>\n",
       "      <td>2.53380</td>\n",
       "      <td>2.66379</td>\n",
       "      <td>1.78517</td>\n",
       "      <td>8.11885</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.340921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390258</th>\n",
       "      <td>27.302300</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.515140</td>\n",
       "      <td>0.043543</td>\n",
       "      <td>0.554265</td>\n",
       "      <td>0.560519</td>\n",
       "      <td>1.633510</td>\n",
       "      <td>1.712510e-15</td>\n",
       "      <td>3.10602</td>\n",
       "      <td>3.09644</td>\n",
       "      <td>2.91947</td>\n",
       "      <td>6.77653</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>1.813766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390259</th>\n",
       "      <td>27.305700</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.515140</td>\n",
       "      <td>0.043541</td>\n",
       "      <td>0.554672</td>\n",
       "      <td>0.560996</td>\n",
       "      <td>1.646360</td>\n",
       "      <td>1.711110e-15</td>\n",
       "      <td>3.10423</td>\n",
       "      <td>3.09293</td>\n",
       "      <td>2.90529</td>\n",
       "      <td>6.77717</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>1.825617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390260</th>\n",
       "      <td>27.309200</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.515140</td>\n",
       "      <td>0.043536</td>\n",
       "      <td>0.555249</td>\n",
       "      <td>0.561109</td>\n",
       "      <td>1.645410</td>\n",
       "      <td>1.710140e-15</td>\n",
       "      <td>3.11372</td>\n",
       "      <td>3.09118</td>\n",
       "      <td>2.89499</td>\n",
       "      <td>6.75728</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>1.824971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390261</th>\n",
       "      <td>27.312500</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.515305</td>\n",
       "      <td>0.043532</td>\n",
       "      <td>0.555029</td>\n",
       "      <td>0.560672</td>\n",
       "      <td>1.640060</td>\n",
       "      <td>1.711640e-15</td>\n",
       "      <td>3.11269</td>\n",
       "      <td>3.09097</td>\n",
       "      <td>2.90542</td>\n",
       "      <td>6.77779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>1.819947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390262</th>\n",
       "      <td>27.315800</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.515305</td>\n",
       "      <td>0.043526</td>\n",
       "      <td>0.554786</td>\n",
       "      <td>0.560474</td>\n",
       "      <td>1.632540</td>\n",
       "      <td>1.710060e-15</td>\n",
       "      <td>3.11065</td>\n",
       "      <td>3.09005</td>\n",
       "      <td>2.92384</td>\n",
       "      <td>6.77203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>1.813038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>390263 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time  Demand 1  Control 1  Output Drive 1  Channel 1  Channel 2  \\\n",
       "0        0.001450  0.125011   0.176033        0.000000   0.211458   0.209182   \n",
       "1        0.001450  0.125011   0.176033        0.000000   0.211458   0.209182   \n",
       "2        0.006283  0.125011   0.176033        0.000012   0.206329   0.206513   \n",
       "3        0.009633  0.125011   0.172626        0.000013   0.206351   0.194663   \n",
       "4        0.013200  0.125011   0.172626        0.000014   0.214463   0.214489   \n",
       "...           ...       ...        ...             ...        ...        ...   \n",
       "390258  27.302300  0.500044   0.515140        0.043543   0.554265   0.560519   \n",
       "390259  27.305700  0.500044   0.515140        0.043541   0.554672   0.560996   \n",
       "390260  27.309200  0.500044   0.515140        0.043536   0.555249   0.561109   \n",
       "390261  27.312500  0.500044   0.515305        0.043532   0.555029   0.560672   \n",
       "390262  27.315800  0.500044   0.515305        0.043526   0.554786   0.560474   \n",
       "\n",
       "        Channel 3     Channel 4  Channel 1 Kurtosis  Channel 2 Kurtosis  \\\n",
       "0        0.145823  1.624200e-15             2.52457             2.94874   \n",
       "1        0.145823  1.624200e-15             2.52457             2.94874   \n",
       "2        0.150478  1.643320e-15             2.32290             2.46553   \n",
       "3        0.148313  1.528270e-15             2.26458             2.55488   \n",
       "4        0.155652  1.791370e-15             2.53380             2.66379   \n",
       "...           ...           ...                 ...                 ...   \n",
       "390258   1.633510  1.712510e-15             3.10602             3.09644   \n",
       "390259   1.646360  1.711110e-15             3.10423             3.09293   \n",
       "390260   1.645410  1.710140e-15             3.11372             3.09118   \n",
       "390261   1.640060  1.711640e-15             3.11269             3.09097   \n",
       "390262   1.632540  1.710060e-15             3.11065             3.09005   \n",
       "\n",
       "        Channel 3 Kurtosis  Channel 4 Kurtosis  Rear Input 1  Rear Input 2  \\\n",
       "0                  1.83760             7.52253             0             0   \n",
       "1                  1.83760             7.52253             0             0   \n",
       "2                  1.76090             6.51397             0             0   \n",
       "3                  1.79255             4.13510             0             0   \n",
       "4                  1.78517             8.11885             0             0   \n",
       "...                    ...                 ...           ...           ...   \n",
       "390258             2.91947             6.77653             0             0   \n",
       "390259             2.90529             6.77717             0             0   \n",
       "390260             2.89499             6.75728             0             0   \n",
       "390261             2.90542             6.77779             0             0   \n",
       "390262             2.92384             6.77203             0             0   \n",
       "\n",
       "        Rear Input 3  Rear Input 4  Rear Input 5  Rear Input 6  Rear Input 7  \\\n",
       "0                  0             0             0             0             0   \n",
       "1                  0             0             0             0             0   \n",
       "2                  0             0             0             0             0   \n",
       "3                  0             0             0             0             0   \n",
       "4                  0             0             0             0             0   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "390258             0             0             0             0             0   \n",
       "390259             0             0             0             0             0   \n",
       "390260             0             0             0             0             0   \n",
       "390261             0             0             0             0             0   \n",
       "390262             0             0             0             0             0   \n",
       "\n",
       "        Rear Input 8  FAULT   RPM  HUMIDITY%  TEMPERATURE_Celsius  \\\n",
       "0                  0      1  1000          0                  -10   \n",
       "1                  0      1  1000          0                  -10   \n",
       "2                  0      1  1000          0                  -10   \n",
       "3                  0      1  1000          0                  -10   \n",
       "4                  0      1  1000          0                  -10   \n",
       "...              ...    ...   ...        ...                  ...   \n",
       "390258             0      0  2000         50                   45   \n",
       "390259             0      0  2000         50                   45   \n",
       "390260             0      0  2000         50                   45   \n",
       "390261             0      0  2000         50                   45   \n",
       "390262             0      0  2000         50                   45   \n",
       "\n",
       "        resultant_vibration_magnitude  \n",
       "0                            0.331264  \n",
       "1                            0.331264  \n",
       "2                            0.328425  \n",
       "3                            0.320111  \n",
       "4                            0.340921  \n",
       "...                               ...  \n",
       "390258                       1.813766  \n",
       "390259                       1.825617  \n",
       "390260                       1.824971  \n",
       "390261                       1.819947  \n",
       "390262                       1.813038  \n",
       "\n",
       "[390263 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\Repositories\\PhD\\parsed_data\\ICE_data\\internal_combustion_engine_bearings.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([390263, 1])\n",
      "torch.Size([390263, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df['resultant_vibration_magnitude'].values.reshape(-1, 1)\n",
    "y = df['FAULT'].values\n",
    "\n",
    "# Normalize input data\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Convert normalized data to PyTorch tensors\n",
    "X_torch = torch.tensor(X_normalized, dtype=torch.float32)\n",
    "y_torch = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)  # Ensure correct shape\n",
    "\n",
    "\n",
    "print(X_torch.shape)\n",
    "print(y_torch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_torch, y_torch, test_size=0.30, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "# Create DataLoader batches\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([273184, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)  # Should return a CUDA version if installed correctly\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2314b32d520>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x = X, y = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall experimenting script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6_16_relu - Epoch 1/50 - Train Loss: 0.6101, Val Loss: 0.6054\n",
      "Model 6_16_relu - Epoch 2/50 - Train Loss: 0.6020, Val Loss: 0.5994\n",
      "Model 6_16_relu - Epoch 3/50 - Train Loss: 0.6001, Val Loss: 0.5983\n",
      "Model 6_16_relu - Epoch 4/50 - Train Loss: 0.5984, Val Loss: 0.5977\n",
      "Model 6_16_relu - Epoch 5/50 - Train Loss: 0.5976, Val Loss: 0.5956\n",
      "Model 6_16_relu - Epoch 6/50 - Train Loss: 0.5973, Val Loss: 0.5997\n",
      "Model 6_16_relu - Epoch 7/50 - Train Loss: 0.5965, Val Loss: 0.6028\n",
      "Model 6_16_relu - Epoch 8/50 - Train Loss: 0.5966, Val Loss: 0.5957\n",
      "Model 6_16_relu - Epoch 9/50 - Train Loss: 0.5962, Val Loss: 0.5973\n",
      "Model 6_16_relu - Epoch 10/50 - Train Loss: 0.5960, Val Loss: 0.5958\n",
      "Early stopping triggered for model 6_16_relu.\n",
      "Model: 6_16_relu took 276.02059149742126seconds\n",
      "Model 6_16_sigmoid - Epoch 1/50 - Train Loss: 0.6789, Val Loss: 0.6765\n",
      "Model 6_16_sigmoid - Epoch 2/50 - Train Loss: 0.6767, Val Loss: 0.6763\n",
      "Model 6_16_sigmoid - Epoch 3/50 - Train Loss: 0.6764, Val Loss: 0.6767\n",
      "Model 6_16_sigmoid - Epoch 4/50 - Train Loss: 0.6763, Val Loss: 0.6767\n",
      "Model 6_16_sigmoid - Epoch 5/50 - Train Loss: 0.6763, Val Loss: 0.6765\n",
      "Model 6_16_sigmoid - Epoch 6/50 - Train Loss: 0.6764, Val Loss: 0.6763\n",
      "Model 6_16_sigmoid - Epoch 7/50 - Train Loss: 0.6763, Val Loss: 0.6765\n",
      "Early stopping triggered for model 6_16_sigmoid.\n",
      "Model: 6_16_sigmoid took 192.81925010681152seconds\n",
      "Model 6_16_tanh - Epoch 1/50 - Train Loss: 0.6205, Val Loss: 0.6022\n",
      "Model 6_16_tanh - Epoch 2/50 - Train Loss: 0.6088, Val Loss: 0.6317\n",
      "Model 6_16_tanh - Epoch 3/50 - Train Loss: 0.6101, Val Loss: 0.5992\n",
      "Model 6_16_tanh - Epoch 4/50 - Train Loss: 0.6095, Val Loss: 0.6110\n",
      "Model 6_16_tanh - Epoch 5/50 - Train Loss: 0.6156, Val Loss: 0.6337\n",
      "Model 6_16_tanh - Epoch 6/50 - Train Loss: 0.6400, Val Loss: 0.6249\n",
      "Model 6_16_tanh - Epoch 7/50 - Train Loss: 0.6545, Val Loss: 0.6827\n",
      "Model 6_16_tanh - Epoch 8/50 - Train Loss: 0.6849, Val Loss: 0.6928\n",
      "Early stopping triggered for model 6_16_tanh.\n",
      "Model: 6_16_tanh took 214.79530811309814seconds\n",
      "Model 6_64_relu - Epoch 1/50 - Train Loss: 0.6160, Val Loss: 0.5998\n",
      "Model 6_64_relu - Epoch 2/50 - Train Loss: 0.6022, Val Loss: 0.6008\n",
      "Model 6_64_relu - Epoch 3/50 - Train Loss: 0.5994, Val Loss: 0.6040\n",
      "Model 6_64_relu - Epoch 4/50 - Train Loss: 0.5980, Val Loss: 0.5965\n",
      "Model 6_64_relu - Epoch 5/50 - Train Loss: 0.5972, Val Loss: 0.5953\n",
      "Model 6_64_relu - Epoch 6/50 - Train Loss: 0.5975, Val Loss: 0.5990\n",
      "Model 6_64_relu - Epoch 7/50 - Train Loss: 0.5971, Val Loss: 0.5945\n",
      "Model 6_64_relu - Epoch 8/50 - Train Loss: 0.5970, Val Loss: 0.5985\n",
      "Model 6_64_relu - Epoch 9/50 - Train Loss: 0.5963, Val Loss: 0.5958\n",
      "Model 6_64_relu - Epoch 10/50 - Train Loss: 0.5964, Val Loss: 0.5946\n",
      "Model 6_64_relu - Epoch 11/50 - Train Loss: 0.5969, Val Loss: 0.5977\n",
      "Model 6_64_relu - Epoch 12/50 - Train Loss: 0.5971, Val Loss: 0.5949\n",
      "Early stopping triggered for model 6_64_relu.\n",
      "Model: 6_64_relu took 323.4838128089905seconds\n",
      "Model 6_64_sigmoid - Epoch 1/50 - Train Loss: 0.6812, Val Loss: 0.6862\n",
      "Model 6_64_sigmoid - Epoch 2/50 - Train Loss: 0.6788, Val Loss: 0.6777\n",
      "Model 6_64_sigmoid - Epoch 3/50 - Train Loss: 0.6780, Val Loss: 0.6764\n",
      "Model 6_64_sigmoid - Epoch 4/50 - Train Loss: 0.6770, Val Loss: 0.6763\n",
      "Model 6_64_sigmoid - Epoch 5/50 - Train Loss: 0.6748, Val Loss: 0.6765\n",
      "Model 6_64_sigmoid - Epoch 6/50 - Train Loss: 0.6766, Val Loss: 0.6785\n",
      "Model 6_64_sigmoid - Epoch 7/50 - Train Loss: 0.6782, Val Loss: 0.6762\n",
      "Model 6_64_sigmoid - Epoch 8/50 - Train Loss: 0.6803, Val Loss: 0.6876\n",
      "Model 6_64_sigmoid - Epoch 9/50 - Train Loss: 0.6799, Val Loss: 0.6768\n",
      "Model 6_64_sigmoid - Epoch 10/50 - Train Loss: 0.6223, Val Loss: 0.6169\n",
      "Model 6_64_sigmoid - Epoch 11/50 - Train Loss: 0.6075, Val Loss: 0.6116\n",
      "Model 6_64_sigmoid - Epoch 12/50 - Train Loss: 0.6084, Val Loss: 0.6199\n",
      "Model 6_64_sigmoid - Epoch 13/50 - Train Loss: 0.6044, Val Loss: 0.5976\n",
      "Model 6_64_sigmoid - Epoch 14/50 - Train Loss: 0.6068, Val Loss: 0.6073\n",
      "Model 6_64_sigmoid - Epoch 15/50 - Train Loss: 0.6049, Val Loss: 0.5974\n",
      "Model 6_64_sigmoid - Epoch 16/50 - Train Loss: 0.6008, Val Loss: 0.5987\n",
      "Model 6_64_sigmoid - Epoch 17/50 - Train Loss: 0.6008, Val Loss: 0.5986\n",
      "Model 6_64_sigmoid - Epoch 18/50 - Train Loss: 0.6002, Val Loss: 0.6013\n",
      "Model 6_64_sigmoid - Epoch 19/50 - Train Loss: 0.6002, Val Loss: 0.6017\n",
      "Model 6_64_sigmoid - Epoch 20/50 - Train Loss: 0.6006, Val Loss: 0.5966\n",
      "Model 6_64_sigmoid - Epoch 21/50 - Train Loss: 0.5996, Val Loss: 0.6000\n",
      "Model 6_64_sigmoid - Epoch 22/50 - Train Loss: 0.5997, Val Loss: 0.6034\n",
      "Model 6_64_sigmoid - Epoch 23/50 - Train Loss: 0.5993, Val Loss: 0.5978\n",
      "Model 6_64_sigmoid - Epoch 24/50 - Train Loss: 0.5990, Val Loss: 0.6020\n",
      "Model 6_64_sigmoid - Epoch 25/50 - Train Loss: 0.5992, Val Loss: 0.6004\n",
      "Early stopping triggered for model 6_64_sigmoid.\n",
      "Model: 6_64_sigmoid took 656.8887147903442seconds\n",
      "Model 6_64_tanh - Epoch 1/50 - Train Loss: 0.6944, Val Loss: 0.6919\n",
      "Model 6_64_tanh - Epoch 2/50 - Train Loss: 0.6965, Val Loss: 0.6904\n",
      "Model 6_64_tanh - Epoch 3/50 - Train Loss: 0.6956, Val Loss: 0.6896\n",
      "Model 6_64_tanh - Epoch 4/50 - Train Loss: 0.6877, Val Loss: 0.6910\n",
      "Model 6_64_tanh - Epoch 5/50 - Train Loss: 0.6955, Val Loss: 0.7082\n",
      "Model 6_64_tanh - Epoch 6/50 - Train Loss: 0.6963, Val Loss: 0.6928\n",
      "Model 6_64_tanh - Epoch 7/50 - Train Loss: 0.6970, Val Loss: 0.6905\n",
      "Model 6_64_tanh - Epoch 8/50 - Train Loss: 0.6968, Val Loss: 0.7643\n",
      "Early stopping triggered for model 6_64_tanh.\n",
      "Model: 6_64_tanh took 211.87908124923706seconds\n",
      "Model 6_128_relu - Epoch 1/50 - Train Loss: 0.6122, Val Loss: 0.6105\n",
      "Model 6_128_relu - Epoch 2/50 - Train Loss: 0.6027, Val Loss: 0.6014\n",
      "Model 6_128_relu - Epoch 3/50 - Train Loss: 0.6003, Val Loss: 0.6228\n",
      "Model 6_128_relu - Epoch 4/50 - Train Loss: 0.5991, Val Loss: 0.5968\n",
      "Model 6_128_relu - Epoch 5/50 - Train Loss: 0.5991, Val Loss: 0.5950\n",
      "Model 6_128_relu - Epoch 6/50 - Train Loss: 0.5974, Val Loss: 0.6015\n",
      "Model 6_128_relu - Epoch 7/50 - Train Loss: 0.5970, Val Loss: 0.5992\n",
      "Model 6_128_relu - Epoch 8/50 - Train Loss: 0.5974, Val Loss: 0.6164\n",
      "Model 6_128_relu - Epoch 9/50 - Train Loss: 0.5972, Val Loss: 0.5968\n",
      "Model 6_128_relu - Epoch 10/50 - Train Loss: 0.5983, Val Loss: 0.5942\n",
      "Model 6_128_relu - Epoch 11/50 - Train Loss: 0.5966, Val Loss: 0.5984\n",
      "Model 6_128_relu - Epoch 12/50 - Train Loss: 0.5966, Val Loss: 0.6009\n",
      "Model 6_128_relu - Epoch 13/50 - Train Loss: 0.5968, Val Loss: 0.5951\n",
      "Model 6_128_relu - Epoch 14/50 - Train Loss: 0.5981, Val Loss: 0.6055\n",
      "Model 6_128_relu - Epoch 15/50 - Train Loss: 0.5988, Val Loss: 0.6024\n",
      "Early stopping triggered for model 6_128_relu.\n",
      "Model: 6_128_relu took 426.91997718811035seconds\n",
      "Model 6_128_sigmoid - Epoch 1/50 - Train Loss: 0.6817, Val Loss: 0.6776\n",
      "Model 6_128_sigmoid - Epoch 2/50 - Train Loss: 0.6852, Val Loss: 0.6908\n",
      "Model 6_128_sigmoid - Epoch 3/50 - Train Loss: 0.6917, Val Loss: 0.6904\n",
      "Model 6_128_sigmoid - Epoch 4/50 - Train Loss: 0.6919, Val Loss: 0.6922\n",
      "Model 6_128_sigmoid - Epoch 5/50 - Train Loss: 0.6918, Val Loss: 0.6927\n",
      "Model 6_128_sigmoid - Epoch 6/50 - Train Loss: 0.6921, Val Loss: 0.6905\n",
      "Early stopping triggered for model 6_128_sigmoid.\n",
      "Model: 6_128_sigmoid took 164.45769095420837seconds\n",
      "Model 6_128_tanh - Epoch 1/50 - Train Loss: 0.7017, Val Loss: 0.6927\n",
      "Model 6_128_tanh - Epoch 2/50 - Train Loss: 0.7024, Val Loss: 0.6925\n",
      "Model 6_128_tanh - Epoch 3/50 - Train Loss: 0.7020, Val Loss: 0.6920\n",
      "Model 6_128_tanh - Epoch 4/50 - Train Loss: 0.7016, Val Loss: 0.6966\n",
      "Model 6_128_tanh - Epoch 5/50 - Train Loss: 0.7019, Val Loss: 0.7234\n",
      "Model 6_128_tanh - Epoch 6/50 - Train Loss: 0.7017, Val Loss: 0.6911\n",
      "Model 6_128_tanh - Epoch 7/50 - Train Loss: 0.7018, Val Loss: 0.7005\n",
      "Model 6_128_tanh - Epoch 8/50 - Train Loss: 0.7011, Val Loss: 0.6922\n",
      "Model 6_128_tanh - Epoch 9/50 - Train Loss: 0.7019, Val Loss: 0.6907\n",
      "Model 6_128_tanh - Epoch 10/50 - Train Loss: 0.7021, Val Loss: 0.6935\n",
      "Model 6_128_tanh - Epoch 11/50 - Train Loss: 0.7020, Val Loss: 0.6916\n",
      "Model 6_128_tanh - Epoch 12/50 - Train Loss: 0.7016, Val Loss: 0.6910\n",
      "Model 6_128_tanh - Epoch 13/50 - Train Loss: 0.7023, Val Loss: 0.7534\n",
      "Model 6_128_tanh - Epoch 14/50 - Train Loss: 0.7016, Val Loss: 0.7045\n",
      "Early stopping triggered for model 6_128_tanh.\n",
      "Model: 6_128_tanh took 384.53689455986023seconds\n",
      "Model 6_256_relu - Epoch 1/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Model 6_256_relu - Epoch 2/50 - Train Loss: 0.6905, Val Loss: 0.6907\n",
      "Model 6_256_relu - Epoch 3/50 - Train Loss: 0.6905, Val Loss: 0.6906\n",
      "Model 6_256_relu - Epoch 4/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Model 6_256_relu - Epoch 5/50 - Train Loss: 0.6905, Val Loss: 0.6906\n",
      "Model 6_256_relu - Epoch 6/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Model 6_256_relu - Epoch 7/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_relu - Epoch 8/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_relu - Epoch 9/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_relu - Epoch 10/50 - Train Loss: 0.6905, Val Loss: 0.6906\n",
      "Model 6_256_relu - Epoch 11/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_relu - Epoch 12/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_relu - Epoch 13/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Early stopping triggered for model 6_256_relu.\n",
      "Model: 6_256_relu took 383.1179714202881seconds\n",
      "Model 6_256_sigmoid - Epoch 1/50 - Train Loss: 0.6914, Val Loss: 0.6904\n",
      "Model 6_256_sigmoid - Epoch 2/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_sigmoid - Epoch 3/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Model 6_256_sigmoid - Epoch 4/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_sigmoid - Epoch 5/50 - Train Loss: 0.6905, Val Loss: 0.6906\n",
      "Model 6_256_sigmoid - Epoch 6/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_sigmoid - Epoch 7/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_sigmoid - Epoch 8/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_sigmoid - Epoch 9/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_sigmoid - Epoch 10/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_sigmoid - Epoch 11/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Model 6_256_sigmoid - Epoch 12/50 - Train Loss: 0.6905, Val Loss: 0.6906\n",
      "Model 6_256_sigmoid - Epoch 13/50 - Train Loss: 0.6905, Val Loss: 0.6906\n",
      "Model 6_256_sigmoid - Epoch 14/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_256_sigmoid - Epoch 15/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Early stopping triggered for model 6_256_sigmoid.\n",
      "Model: 6_256_sigmoid took 431.4696033000946seconds\n",
      "Model 6_256_tanh - Epoch 1/50 - Train Loss: 0.7098, Val Loss: 0.7027\n",
      "Model 6_256_tanh - Epoch 2/50 - Train Loss: 0.7124, Val Loss: 0.7161\n",
      "Model 6_256_tanh - Epoch 3/50 - Train Loss: 0.7101, Val Loss: 0.7406\n",
      "Model 6_256_tanh - Epoch 4/50 - Train Loss: 0.7109, Val Loss: 0.6939\n",
      "Model 6_256_tanh - Epoch 5/50 - Train Loss: 0.7106, Val Loss: 0.6906\n",
      "Model 6_256_tanh - Epoch 6/50 - Train Loss: 0.7112, Val Loss: 0.6961\n",
      "Model 6_256_tanh - Epoch 7/50 - Train Loss: 0.7106, Val Loss: 0.6920\n",
      "Model 6_256_tanh - Epoch 8/50 - Train Loss: 0.7110, Val Loss: 0.6992\n",
      "Model 6_256_tanh - Epoch 9/50 - Train Loss: 0.7111, Val Loss: 0.7063\n",
      "Model 6_256_tanh - Epoch 10/50 - Train Loss: 0.7128, Val Loss: 0.7091\n",
      "Early stopping triggered for model 6_256_tanh.\n",
      "Model: 6_256_tanh took 289.4987816810608seconds\n",
      "Model 6_512_relu - Epoch 1/50 - Train Loss: 0.6181, Val Loss: 0.6126\n",
      "Model 6_512_relu - Epoch 2/50 - Train Loss: 0.6029, Val Loss: 0.6211\n",
      "Model 6_512_relu - Epoch 3/50 - Train Loss: 0.6008, Val Loss: 0.5999\n",
      "Model 6_512_relu - Epoch 4/50 - Train Loss: 0.5995, Val Loss: 0.5974\n",
      "Model 6_512_relu - Epoch 5/50 - Train Loss: 0.5991, Val Loss: 0.5951\n",
      "Model 6_512_relu - Epoch 6/50 - Train Loss: 0.5995, Val Loss: 0.6018\n",
      "Model 6_512_relu - Epoch 7/50 - Train Loss: 0.5994, Val Loss: 0.6044\n",
      "Model 6_512_relu - Epoch 8/50 - Train Loss: 0.5991, Val Loss: 0.6003\n",
      "Model 6_512_relu - Epoch 9/50 - Train Loss: 0.5988, Val Loss: 0.5985\n",
      "Model 6_512_relu - Epoch 10/50 - Train Loss: 0.5985, Val Loss: 0.5994\n",
      "Early stopping triggered for model 6_512_relu.\n",
      "Model: 6_512_relu took 308.1656422615051seconds\n",
      "Model 6_512_sigmoid - Epoch 1/50 - Train Loss: 0.6968, Val Loss: 0.6905\n",
      "Model 6_512_sigmoid - Epoch 2/50 - Train Loss: 0.6907, Val Loss: 0.6904\n",
      "Model 6_512_sigmoid - Epoch 3/50 - Train Loss: 0.6906, Val Loss: 0.6906\n",
      "Model 6_512_sigmoid - Epoch 4/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_512_sigmoid - Epoch 5/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_512_sigmoid - Epoch 6/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Model 6_512_sigmoid - Epoch 7/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Model 6_512_sigmoid - Epoch 8/50 - Train Loss: 0.6905, Val Loss: 0.6906\n",
      "Model 6_512_sigmoid - Epoch 9/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Model 6_512_sigmoid - Epoch 10/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Early stopping triggered for model 6_512_sigmoid.\n",
      "Model: 6_512_sigmoid took 296.74802470207214seconds\n",
      "Model 6_512_tanh - Epoch 1/50 - Train Loss: 0.7262, Val Loss: 0.6912\n",
      "Model 6_512_tanh - Epoch 2/50 - Train Loss: 0.7262, Val Loss: 0.7063\n",
      "Model 6_512_tanh - Epoch 3/50 - Train Loss: 0.7273, Val Loss: 0.6932\n",
      "Model 6_512_tanh - Epoch 4/50 - Train Loss: 0.7269, Val Loss: 0.8101\n",
      "Model 6_512_tanh - Epoch 5/50 - Train Loss: 0.7305, Val Loss: 0.6943\n",
      "Model 6_512_tanh - Epoch 6/50 - Train Loss: 0.7262, Val Loss: 0.6906\n",
      "Model 6_512_tanh - Epoch 7/50 - Train Loss: 0.7254, Val Loss: 0.6912\n",
      "Model 6_512_tanh - Epoch 8/50 - Train Loss: 0.7273, Val Loss: 0.7225\n",
      "Model 6_512_tanh - Epoch 9/50 - Train Loss: 0.7296, Val Loss: 0.7614\n",
      "Model 6_512_tanh - Epoch 10/50 - Train Loss: 0.7284, Val Loss: 0.6935\n",
      "Model 6_512_tanh - Epoch 11/50 - Train Loss: 0.7275, Val Loss: 0.6919\n",
      "Early stopping triggered for model 6_512_tanh.\n",
      "Model: 6_512_tanh took 328.9382517337799seconds\n",
      "Model 6_1024_relu - Epoch 1/50 - Train Loss: 0.6952, Val Loss: 0.6904\n",
      "Model 6_1024_relu - Epoch 2/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_1024_relu - Epoch 3/50 - Train Loss: 0.6905, Val Loss: 0.6912\n",
      "Model 6_1024_relu - Epoch 4/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Model 6_1024_relu - Epoch 5/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Model 6_1024_relu - Epoch 6/50 - Train Loss: 0.6905, Val Loss: 0.6907\n",
      "Early stopping triggered for model 6_1024_relu.\n",
      "Model: 6_1024_relu took 225.11574006080627seconds\n",
      "Model 6_1024_sigmoid - Epoch 1/50 - Train Loss: 0.7020, Val Loss: 0.6905\n",
      "Model 6_1024_sigmoid - Epoch 2/50 - Train Loss: 0.6909, Val Loss: 0.6909\n",
      "Model 6_1024_sigmoid - Epoch 3/50 - Train Loss: 0.6906, Val Loss: 0.6904\n",
      "Model 6_1024_sigmoid - Epoch 4/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_1024_sigmoid - Epoch 5/50 - Train Loss: 0.6906, Val Loss: 0.6904\n",
      "Model 6_1024_sigmoid - Epoch 6/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 6_1024_sigmoid - Epoch 7/50 - Train Loss: 0.6905, Val Loss: 0.6907\n",
      "Model 6_1024_sigmoid - Epoch 8/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Early stopping triggered for model 6_1024_sigmoid.\n",
      "Model: 6_1024_sigmoid took 300.5666823387146seconds\n",
      "Model 6_1024_tanh - Epoch 1/50 - Train Loss: 0.7560, Val Loss: 0.6966\n",
      "Model 6_1024_tanh - Epoch 2/50 - Train Loss: 0.7537, Val Loss: 0.7093\n",
      "Model 6_1024_tanh - Epoch 3/50 - Train Loss: 0.7580, Val Loss: 0.7251\n",
      "Model 6_1024_tanh - Epoch 4/50 - Train Loss: 0.7582, Val Loss: 0.7082\n",
      "Model 6_1024_tanh - Epoch 5/50 - Train Loss: 0.7526, Val Loss: 0.6914\n",
      "Model 6_1024_tanh - Epoch 6/50 - Train Loss: 0.7579, Val Loss: 0.6967\n",
      "Model 6_1024_tanh - Epoch 7/50 - Train Loss: 0.7574, Val Loss: 0.6917\n",
      "Model 6_1024_tanh - Epoch 8/50 - Train Loss: 0.7574, Val Loss: 0.7433\n",
      "Model 6_1024_tanh - Epoch 9/50 - Train Loss: 0.7537, Val Loss: 0.7286\n",
      "Model 6_1024_tanh - Epoch 10/50 - Train Loss: 0.7574, Val Loss: 0.7043\n",
      "Early stopping triggered for model 6_1024_tanh.\n",
      "Model: 6_1024_tanh took 376.59488582611084seconds\n",
      "Model 22_16_relu - Epoch 1/50 - Train Loss: 0.6907, Val Loss: 0.6905\n",
      "Model 22_16_relu - Epoch 2/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 22_16_relu - Epoch 3/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 22_16_relu - Epoch 4/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 22_16_relu - Epoch 5/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 22_16_relu - Epoch 6/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Model 22_16_relu - Epoch 7/50 - Train Loss: 0.6906, Val Loss: 0.6904\n",
      "Model 22_16_relu - Epoch 8/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 22_16_relu - Epoch 9/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Early stopping triggered for model 22_16_relu.\n",
      "Model: 22_16_relu took 597.1859016418457seconds\n",
      "Model 22_16_sigmoid - Epoch 1/50 - Train Loss: 0.6907, Val Loss: 0.6917\n",
      "Model 22_16_sigmoid - Epoch 2/50 - Train Loss: 0.6905, Val Loss: 0.6905\n",
      "Model 22_16_sigmoid - Epoch 3/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 22_16_sigmoid - Epoch 4/50 - Train Loss: 0.6905, Val Loss: 0.6904\n",
      "Model 22_16_sigmoid - Epoch 5/50 - Train Loss: 0.6905, Val Loss: 0.6906\n",
      "Model 22_16_sigmoid - Epoch 6/50 - Train Loss: 0.6905, Val Loss: 0.6908\n",
      "Model 22_16_sigmoid - Epoch 7/50 - Train Loss: 0.6905, Val Loss: 0.6906\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m     68\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y_batch)\n\u001b[1;32m---> 69\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     70\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     71\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\HackoAcademic_Anaconda\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32md:\\HackoAcademic_Anaconda\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32md:\\HackoAcademic_Anaconda\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define hyperparameters\n",
    "hidden_layer_numbers = [2, 6, 22, 52, 102, 202, 502]\n",
    "neurons_per_layer = [4, 8, 16, 64, 128, 256, 512, 1024]\n",
    "activation_functions = {'relu': nn.ReLU(), 'sigmoid': nn.Sigmoid(), 'tanh': nn.Tanh()}\n",
    "\n",
    "# Generate all possible combinations\n",
    "param_combinations = list(itertools.product(hidden_layer_numbers, neurons_per_layer, activation_functions.keys()))\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Define the ANN Model\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, neurons_per_layer, activation_function):\n",
    "        super(ANN, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, neurons_per_layer))\n",
    "        layers.append(activation_function)\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(hidden_layers - 1):  # Exclude input layer\n",
    "            layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
    "            layers.append(activation_function)\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(neurons_per_layer, 1))  # Binary classification\n",
    "        layers.append(nn.Sigmoid())  # Ensures output is in [0,1]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Training loop with validation checks\n",
    "num_epochs = 50\n",
    "early_stopping_threshold = 5  # Stop if validation loss doesn't improve for 5 epochs\n",
    "\n",
    "for hidden_layers, neurons, activation in param_combinations:\n",
    "    activation_fn = activation_functions[activation]\n",
    "\n",
    "    # Create the model\n",
    "    model = ANN(X_train.shape[1], hidden_layers, neurons, activation_fn)\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    model.to(device)\n",
    "\n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions.to(device), y_batch.to(device))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Model {hidden_layers}_{neurons}_{activation} - Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= early_stopping_threshold:\n",
    "            print(f\"Early stopping triggered for model {hidden_layers}_{neurons}_{activation}.\")\n",
    "            break\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    model_key = f\"{hidden_layers}_{neurons}_{activation}\"\n",
    "    print(f\"Model: {model_key} took {elapsed_time/60}minutes.\")\n",
    "\n",
    "    # Store results\n",
    "    results[model_key] = {\n",
    "        \"time_taken\": elapsed_time,\n",
    "        \"num_of_epochs\": epoch + 1,\n",
    "        \"model\": model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate best model on test data\n",
    "best_model_key = min(results, key=lambda k: results[k][\"time_taken\"])  # Selecting the fastest model\n",
    "best_model = results[best_model_key][\"model\"]\n",
    "\n",
    "best_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        predictions = best_model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Convert probabilities to binary labels\n",
    "        predicted_labels = (predictions > 0.5).float()\n",
    "        correct += (predicted_labels == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(f\"Best Model: {best_model_key} - Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([58539, 1, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing for CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_dataloaders(X_series: pd.Series, y_series: pd.Series, window_size: int = 10, stride: int = 1, batch_size: int = 256, train_ratio: float = 0.7, val_ratio: float = 0.15):\n",
    "    \"\"\"\n",
    "    Prepares PyTorch DataLoaders from a pandas Series.\n",
    "    \n",
    "    1. Normalizes X using Min-Max Scaling.\n",
    "    2. Converts X and y to torch tensors.\n",
    "    3. Reshapes X into (N, window_size, 1) for CNN + LSTM models.\n",
    "    4. Splits into train, validation, and test sets.\n",
    "    5. Returns PyTorch DataLoaders.\n",
    "\n",
    "    Args:\n",
    "        X_series (pd.Series): Input feature series.\n",
    "        y_series (pd.Series): Target labels series.\n",
    "        window_size (int, optional): Length of each sequence window. Defaults to 10.\n",
    "        stride (int, optional): Step size for windowing. Defaults to 1.\n",
    "        batch_size (int, optional): Batch size for DataLoaders. Defaults to 32.\n",
    "        train_ratio (float, optional): Percentage of data for training. Defaults to 0.7.\n",
    "        val_ratio (float, optional): Percentage of data for validation. Defaults to 0.15.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: Train DataLoader.\n",
    "        DataLoader: Validation DataLoader.\n",
    "        DataLoader: Test DataLoader.\n",
    "    \"\"\"\n",
    "    # Normalize X using Min-Max Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_series.values.reshape(-1, 1))  # Ensure shape (N, 1)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_series.values, dtype=torch.long)  # Assuming classification task\n",
    "\n",
    "    # Apply rolling window transformation\n",
    "    X_windows = X_tensor.unfold(dimension=0, size=window_size, step=stride)\n",
    "    X_windows = X_windows.permute(0, 2, 1)  # Reshape to (N, window_size, 1)\n",
    "    \n",
    "    # Align labels: Take the last label in each sequence\n",
    "    y_windows = y_tensor[window_size - 1::stride]\n",
    "\n",
    "    # Determine split indices\n",
    "    total_samples = X_windows.shape[0]\n",
    "    train_end = int(train_ratio * total_samples)\n",
    "    val_end = train_end + int(val_ratio * total_samples)\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    X_train, y_train = X_windows[:train_end], y_windows[:train_end]\n",
    "    X_val, y_val = X_windows[train_end:val_end], y_windows[train_end:val_end]\n",
    "    X_test, y_test = X_windows[val_end:], y_windows[val_end:]\n",
    "\n",
    "    # Create TensorDatasets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = prepare_dataloaders(df['resultant_vibration_magnitude'], df['FAULT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2310bec1910>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FAULT\n",
       "1    209662\n",
       "0    180601\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FAULT'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN_LSTM_FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=1, num_classes=2, dropout=0.3):\n",
    "        super(CNN_LSTM_FeatureExtractor, self).__init__()\n",
    "\n",
    "        # CNN Feature Extractor\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)  # Batch Normalization\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Compute reduced sequence length after CNN + pooling\n",
    "        reduced_seq_len = input_size // 2  # Since we use only one pooling layer\n",
    "\n",
    "        # LSTM for temporal features (unidirectional)\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Feature representation layer\n",
    "        self.feature_layer = nn.Linear(hidden_size, 128)\n",
    "        self.dropout = nn.Dropout(dropout)  # Regularization\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Change shape to (Batch, Channels, Seq_Len) for CNN\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # Apply Conv1D + BatchNorm + ReLU\n",
    "        x = self.pool(x)  # MaxPool\n",
    "        x = self.relu(self.bn2(self.conv2(x)))  # Apply Conv1D + BatchNorm + ReLU\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # Reshape to (Batch, Seq_Len, Features) for LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        features = self.feature_layer(x[:, -1, :])  # Take last time step features\n",
    "        features = self.dropout(features)  # Apply dropout\n",
    "        output = self.classifier(features)  # Classification output\n",
    "\n",
    "        return output, features  # Return both classification & extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== TRAINING FUNCTION ======\n",
    "def train_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.01, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Trains the CNN + LSTM model using provided DataLoaders.\n",
    "    \n",
    "    Args:\n",
    "        model: CNN_LSTM model instance.\n",
    "        train_loader: DataLoader for training data.\n",
    "        val_loader: DataLoader for validation data.\n",
    "        num_epochs: Number of epochs to train.\n",
    "        learning_rate: Learning rate for optimizer.\n",
    "        device: Device to run training on ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                outputs, _ = model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ====== MODEL TRAINING FUNCTION ======\n",
    "def train_and_evaluate(train_loader, val_loader, input_size, num_classes, num_epochs=20):\n",
    "    \"\"\"\n",
    "    Initializes, trains, and evaluates the CNN + LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        train_loader: Training DataLoader.\n",
    "        val_loader: Validation DataLoader.\n",
    "        input_size: Sequence length for LSTM input.\n",
    "        num_classes: Number of output classes.\n",
    "        num_epochs: Number of epochs to train.\n",
    "    \"\"\"\n",
    "    model = CNN_LSTM_FeatureExtractor(input_size=input_size, num_classes=num_classes)\n",
    "    trained_model = train_model(model, train_loader, val_loader, num_epochs=num_epochs)\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 525.1954, Val Loss: 682.1326, Val Acc: 0.00%\n",
      "Epoch [2/20], Loss: 520.4815, Val Loss: 773.6178, Val Acc: 0.00%\n",
      "Epoch [3/20], Loss: 518.2557, Val Loss: 719.1965, Val Acc: 0.00%\n",
      "Epoch [4/20], Loss: 515.9782, Val Loss: 1151.6598, Val Acc: 0.06%\n",
      "Epoch [5/20], Loss: 514.8540, Val Loss: 1106.7001, Val Acc: 0.00%\n",
      "Epoch [6/20], Loss: 511.7287, Val Loss: 1054.5476, Val Acc: 0.01%\n",
      "Epoch [7/20], Loss: 508.9003, Val Loss: 1234.9418, Val Acc: 0.10%\n",
      "Epoch [8/20], Loss: 506.2995, Val Loss: 1584.4428, Val Acc: 0.01%\n",
      "Epoch [9/20], Loss: 505.7672, Val Loss: 1345.0897, Val Acc: 0.05%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train_and_evaluate(train_loader, val_loader, input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAULT\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()))\n",
      "Cell \u001b[1;32mIn[62], line 68\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(train_loader, val_loader, input_size, num_classes, num_epochs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03mInitializes, trains, and evaluates the CNN + LSTM model.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    num_epochs: Number of epochs to train.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN_LSTM_FeatureExtractor(input_size\u001b[38;5;241m=\u001b[39minput_size, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[1;32m---> 68\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, val_loader, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_model\n",
      "Cell \u001b[1;32mIn[62], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, learning_rate, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     20\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     23\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\HackoAcademic_Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32md:\\HackoAcademic_Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\HackoAcademic_Anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32md:\\HackoAcademic_Anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32md:\\HackoAcademic_Anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\HackoAcademic_Anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32md:\\HackoAcademic_Anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = train_and_evaluate(train_loader, val_loader, input_size=10, num_classes=len(df['FAULT'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
